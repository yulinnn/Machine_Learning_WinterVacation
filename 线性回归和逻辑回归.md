# 线性回归

设有线性方程：$$y=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$$。
设$$x_0=1$$，则有：$$y=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$$，即：$$y=\theta^TX$$。
## 正规方程法

### 损失函数

采用平方损失函数。
$$
J(\theta)=\frac{1}{m}\sum_{i=0}^m(y_{pred}-y_{true})^2
\\Where\ y_{pred}=\theta^TX
$$
### 核心公式

$$
\theta=(X^TX)^{-1}X^Ty
$$
### 代码实现

```python
def regular_expression(data_x,data_y):
    data_x = np.hstack((np.ones((len(data_x), 1)), data_x))#添加一列x0=1
    theta = np.linalg.inv(data_x.T.dot(data_x)).dot(data_x.T.dot(data_y.T))
    #损失函数
    pred = np.sum(theta * data_x,axis = 1)
    loss = np.mean((pred - data_y) ** 2)
    return loss,theta
```
### 优缺点

优点：不需要选择学习率，不需要多次迭代，在特征参数较少时运算速度快。
缺点：由于需要计算$$(X^TX)^{-1}$$，当特征参数n相当大时运算代价很高（时间复杂度：$$O(n^3)$$），导致计算缓慢。且正规方程法只适用于线性回归模型，不能用于逻辑回归模型等其他模型。

## 梯度下降法
### 损失函数

同样采用平方损失函数。其中$$\frac{1}{2}$$是为了偏导计算方便，不影响结果。
$$
J(\theta)=\frac{1}{2m}\sum_{i=0}^m(\theta^TX-y_{true})^2
$$
### 核心公式

对于每个特征参数，求其损失函数的偏导（梯度），然后采用梯度下降法更新$$\theta$$（其中$$\alpha$$为学习率）。在设定的迭代次数内，重复上述步骤。
$$
\frac{dJ(\theta)}{d\theta}=\frac{1}{m}\sum_{i=0}^m(\theta^TX-y_{true})X
\\\theta=\theta-\alpha\frac{dJ(\theta)}{d\theta}
$$
### 代码实现

```python
def gradient_descent(data_x,data_y,MAX_LOOP = 10001,learning_rate = 1.9):
    num_of_feature = len(data_x[0])
    theta = np.zeros((1,num_of_feature + 1))#初始化theta为0
    data_x = np.hstack((np.ones((len(data_x),1)),data_x))#添加一列x0=1
    LOSS = []
    for i in range(MAX_LOOP):
        gradient = np.zeros((1,num_of_feature + 1))
        for j in range(num_of_feature + 1):
            gradient[0,j] = np.sum((np.sum(theta * data_x,axis = 1) - data_y) * data_x[:,j].T) / len(data_x)
        theta = theta - learning_rate * gradient
        #损失函数，每500轮记录一次
        if i % 500 == 0 and i != 0:
            pred = np.sum(theta * data_x,axis = 1)
            loss = np.mean((pred - data_y) ** 2)
            LOSS.append(loss)
    return LOSS,theta
```
### 优缺点

优点：当特征参数数量大时也能较好适用（时间复杂度：$$O(kn^2)$$），且适用于各种模型。
缺点：需要选择学习率，且需要多次迭代，在面对特征参数较少的情况下计算速度缓慢。
# 逻辑回归

逻辑回归不是“回归”，而是用回归的方法做分类任务。由于采用梯度下降法求解参数，其实现方法与线性回归的梯度下降法类似，需要确定学习率与迭代次数。以下只讨论二分类。

### 核心公式与预测方法

$$
\frac{dJ(\theta)}{d\theta}=\frac{1}{m}\sum_{i=0}^m(Sigmoid(\theta^TX)-y_{true})X
\\\theta=\theta-\alpha\frac{dJ(\theta)}{d\theta}
\\Where\ Sigmoid(x)=\frac{1}{1+e^{-x}}
$$

Sigmoid函数即逻辑函数，其取值在(0,1)之间，具有概率意义，即一个值预测为正例的概率。
$$
P(y_{i}=0)=1-Sigmoid(\theta^TX)
\\P(y_{i}=1)=Sigmoid(\theta^TX)
\\y_{i}为某条样本的预测值，取值范围为\{0,1\}，0代表负例，1代表正例
$$
在选定阈值后，则可用训练出的theta，通过逻辑函数输出的结果判断样本为正例或是负例（例如取阈值为0.5，则输出大于0.5的样本是正例，小于0.5的是负例），从而达到分类的效果。

### 交叉熵损失函数

交叉熵损失函数可以用于判断某个样本的预测分布与真实分布的接近程度，预测分布越接近真实分布，交叉熵损失越小。某一组测试数据的预测结果的交叉熵损失函数的平均值的大小，可以反映出训练出的theta的预测效果的好坏。
$$
J(\theta)=\frac{1}{m}\sum_{i=i}^{m}-y_ilog(Sigmoid(\theta^Tx_i))-(1-y_i)log(1-Sigmoid(\theta^Tx_i))
\\For\ y_i\ in\ \{0,1\}
$$




